{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/nicho206/.local/lib/python3.11/site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/nicho206/.local/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/nicho206/.local/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nicho206/.local/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in /home/nicho206/.local/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/nicho206/.local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/nicho206/.local/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nicho206/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /apps/spack/negishi/apps/anaconda/2024.02-py311-gcc-8.5.0-lr57z2f/lib/python3.11/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install torch\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/nicho206/.local/lib/python3.11/site-packages\")\n",
    "import json\n",
    "import faiss\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRYPATH = \"/home/nicho206/ScholarSearch/ml-papers.txt\"\n",
    "MODELTYPE = \"allenai/scibert_scivocab_uncased\"\n",
    "EMBEDDING_DIM = 768 # embedding.pooler_output.shape[0]\n",
    "INDEX_PATH = \"/home/nicho206/ScholarSearch/papers.index\"\n",
    "INDEX_METADATA_PATH = \"/home/nicho206/ScholarSearch/papers.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b536e9e941694e76ae324617ba398cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9707018e8ede49e2b10382a16f82f173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5bf11eef1c41aa99020d54f4034984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODELTYPE)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "index_metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "abstract_index = faiss.IndexFlatL2(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_entry(entry):\n",
    "    tokenized = tokenizer(entry, return_tensors=\"pt\")\n",
    "    return tokenized\n",
    "\n",
    "def generate_chunks(encoded):\n",
    "    chunk_size = 512  # BERT's maximum input size\n",
    "\n",
    "    input_ids = encoded['input_ids'].squeeze(0)  \n",
    "    attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "\n",
    "    input_ids_chunks = []\n",
    "    attention_mask_chunks = []\n",
    "\n",
    "    for i in range(0, len(input_ids), chunk_size - 2):  # Account for special tokens\n",
    "        input_ids_chunk = input_ids[i:i + chunk_size - 2]\n",
    "        attention_mask_chunk = attention_mask[i:i + chunk_size - 2]\n",
    "\n",
    "        input_ids_chunk = torch.cat([\n",
    "            torch.tensor([tokenizer.cls_token_id]),  # [CLS] at the beginning\n",
    "            input_ids_chunk,\n",
    "            torch.tensor([tokenizer.sep_token_id])  # [SEP] at the end\n",
    "        ])\n",
    "        attention_mask_chunk = torch.cat([\n",
    "            torch.tensor([1]),  # Attention mask for [CLS]\n",
    "            attention_mask_chunk,\n",
    "            torch.tensor([1])  # Attention mask for [SEP]\n",
    "        ])\n",
    "\n",
    "        padding_length = chunk_size - input_ids_chunk.size(0)\n",
    "        if padding_length > 0:\n",
    "            input_ids_chunk = torch.cat([input_ids_chunk, torch.zeros(padding_length, dtype=torch.long)])\n",
    "            attention_mask_chunk = torch.cat([attention_mask_chunk, torch.zeros(padding_length, dtype=torch.long)])\n",
    "\n",
    "        input_ids_chunks.append(input_ids_chunk.unsqueeze(0))  # Add batch dimension\n",
    "        attention_mask_chunks.append(attention_mask_chunk.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "    input_ids_chunks = torch.cat(input_ids_chunks, dim=0)\n",
    "    attention_mask_chunks = torch.cat(attention_mask_chunks, dim=0)\n",
    "\n",
    "    return input_ids_chunks, attention_mask_chunks\n",
    "\n",
    "\n",
    "def generate_chunks_embedding(input_chunks, attention_chunks):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_chunk, attention_chunk in zip(input_chunks, attention_chunks): # tensor [512] & tensor [512]\n",
    "            embeddings.append(model(input_ids=input_chunk.unsqueeze(0), attention_mask=attention_chunk.unsqueeze(0))# Model expects [1,512] shape\n",
    "                            .pooler_output[0]) # pooler_output.shape = [1, 768]\n",
    "\n",
    "    if len(embeddings) == 1: \n",
    "        return embeddings[0]\n",
    "\n",
    "    return torch.mean(torch.stack(embeddings), dim=0)       \n",
    "\n",
    "def generate_embedding(encoded_input):\n",
    "    encoded_input.input_ids = encoded_input.input_ids[:, 0:512]\n",
    "    encoded_input.attention_mask = encoded_input.attention_mask[:, 0:512]\n",
    "    with torch.no_grad():\n",
    "        embedding = model(**encoded_input)\n",
    "    return embedding\n",
    "\n",
    "def index_embedding(title_embedding, abstract_embedding, title, id, categories):\n",
    "    # Add the embedding to the indexing system \n",
    "    title_index.add(title_embedding.numpy().reshape(1,-1))\n",
    "    abstract_index.add(abstract_embedding.numpy().reshape(1,-1))\n",
    "    index_metadata.append({\n",
    "        \"id\": id,\n",
    "        \"title\": title,\n",
    "        \"categories\": categories\n",
    "    })\n",
    "\n",
    "def process_batch(batch):\n",
    "    title_embeddings = []\n",
    "    abstract_embeddings = []\n",
    "    metadata = []\n",
    "    for line in batch:\n",
    "        entry = json.loads(line)\n",
    "        encoded_title = tokenize_entry(entry[\"title\"])\n",
    "        encoded_abstract = tokenize_entry(entry[\"abstract\"])\n",
    "        title_embedding = generate_embedding(encoded_title)\n",
    "        abstract_embedding = generate_embedding(encoded_abstract)\n",
    "        title_embeddings.append(title_embedding)\n",
    "        abstract_embeddings.append(abstract_embedding)\n",
    "        metadata.append((entry[\"title\"], entry[\"id\"], entry[\"categories\"]))\n",
    "    return title_embeddings, abstract_embeddings, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches\n",
      "Starting batch processing\n",
      "Submitting batches\n",
      "Waiting for completion\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(ENTRYPATH, \"r\") as f:\n",
    "    print(\"Creating batches\")\n",
    "    batch_size = 25  # Or another size suitable for your dataset\n",
    "    lines = f.readlines()\n",
    "    batches = [lines[i:i + batch_size] for i in range(0, 15000, batch_size)]\n",
    "\n",
    "    print(\"Starting batch processing\")\n",
    "    with ThreadPoolExecutor(12) as executor:\n",
    "        print(\"Submitting batches\")\n",
    "        futures = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        print(\"Waiting for completion\")\n",
    "        for future in as_completed(futures):\n",
    "            title_embeddings, abstract_embeddings, metadata = future.result()\n",
    "            # Now, outside the parallel part, add embeddings to the index\n",
    "            try:\n",
    "                for t_em, ab_em, meta in zip(title_embeddings, abstract_embeddings, metadata):\n",
    "                    index_embedding(t_em, ab_em, *meta)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        print(\"Done\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert index.ntotal == len(index_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the index and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "faiss.write_index(index, \"sci.index\")\n",
    "with open(\"sci_metadata.txt\", \"w\") as f:\n",
    "    for data in index_metadata:\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Image recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_query = tokenizer(query, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model(**encoded_query).pooler_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    D, I = index.search(embedding.numpy().reshape(1,-1), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7189 14411 10773  7980  2993  4423 11237  6952  7846   968  9158  2006\n",
      "   7119  5439 10837  6301 14261  8600  1740  3018]]\n"
     ]
    }
   ],
   "source": [
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Review: Expert System for Diagnosis of Myocardial Infarction\n",
      "Modified Weibull distribution for Biomedical signals denoising\n",
      "Facial Expressions recognition Based on Principal Component Analysis\n",
      "  (PCA)\n",
      "A Review of Image Mosaicing Techniques\n",
      "Cognitive Memory Network\n",
      "Fingertip Detection: A Fast Method with Natural Hand\n",
      "Thinning Algorithm Using Hypergraph Based Morphological Operators\n",
      "Feature Extraction of Human Lip Prints\n",
      "A Comparative study Between Fuzzy Clustering Algorithm and Hard\n",
      "  Clustering Algorithm\n",
      "Swarm Intelligence\n",
      "Similarity- based approach for outlier detection\n",
      "Automatic Extraction of Open Space Area from High Resolution Urban\n",
      "  Satellite Imagery\n",
      "Content Based Image Indexing and Retrieval\n",
      "A Robust Rapid Approach to Image Segmentation with Optimal Thresholding\n",
      "  and Watershed Transform\n",
      "A Review of Feature and Data Fusion with Medical Images\n",
      "Syntactic sensitive complexity for symbol-free sequence\n",
      "Obstacle evasion using fuzzy logic in a sliding blades problem\n",
      "  environment\n",
      "Introduction to Clustering Algorithms and Applications\n",
      "Prunnig Algorithm of Generation a Minimal Set of Rule Reducts Based on\n",
      "  Rough Set Theory\n",
      "Robust seed selection algorithm for k-means type algorithms\n"
     ]
    }
   ],
   "source": [
    "for i in I[0]:\n",
    "    print(index_metadata[i][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
